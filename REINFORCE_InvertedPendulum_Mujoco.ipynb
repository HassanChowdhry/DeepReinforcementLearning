{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjsO7yKMwKqAv2mJXL9687",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HassanChowdhry/DeepReinforcementLearning/blob/main/REINFORCE_InvertedPendulum_Mujoco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training using REINFORCE for Mujoco"
      ],
      "metadata": {
        "id": "24BsIL9qYMoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q gymnasium[mujoco] pyvirtualdisplay\n",
        "from __future__ import annotations\n",
        "import os\n",
        "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.normal import Normal\n",
        "from typing import Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn8aBSVmYL6J",
        "outputId": "1935b4f5-c0c7-430a-b669-058198662b3d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: mujoco>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.3.2)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.2.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (2.9.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.22.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Network"
      ],
      "metadata": {
        "id": "9xHyz-nDzdYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Policy network: π_θ(a|s) ~ N(μ(s), σ²)\n",
        "# ----------------------------------------------------------\n",
        "class Policy(nn.Module):\n",
        "  def __init__(self,\n",
        "               obs_dims: int,\n",
        "               action_dims: int,\n",
        "               hidden_dims: Tuple[int, int] = (32, 32),\n",
        "            ):\n",
        "    \"\"\"Initializes a neural network that estimates the mean and standard deviation\n",
        "      of a normal distribution from which an action is sampled from.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # network\n",
        "    self.input_layer = nn.Linear(obs_dims, hidden_dims[0])\n",
        "    self.hidden_layer = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "    self.mean = nn.Linear(hidden_dims[1], action_dims)\n",
        "    self.std = nn.Parameter(torch.full((action_dims,), -0.5))\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    \"\"\"Conditioned on the observation, returns the mean and standard deviation\n",
        "      of a normal distribution from which an action is sampled from.\n",
        "\n",
        "    Args:\n",
        "        x: Observation from the environment\n",
        "\n",
        "    Returns:\n",
        "        action_means: predicted mean of the normal distribution\n",
        "        action_stddevs: predicted standard deviation of the normal distribution\n",
        "    \"\"\"\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "      x = torch.tensor(x, dtype=torch.float32)\n",
        "      x = x.unsqueeze(0)\n",
        "\n",
        "    x = self.activation(self.input_layer(x))\n",
        "    x = self.activation(self.hidden_layer(x))\n",
        "    mean = self.mean(x)\n",
        "    std = torch.exp(self.std)\n",
        "\n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "5lmdw1JQo4MD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an agent"
      ],
      "metadata": {
        "id": "MDbCFkHEzZ82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import log\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class REINFORCE:\n",
        "  def __init__(self, observation_space, action_space, gamma=0.99, lr=3e-4, epsilon=1e-6):\n",
        "    \"\"\"Initializes an agent that learns a policy via REINFORCE algorithm [1]\n",
        "    to solve the task at hand (Inverted Pendulum v4).\n",
        "\n",
        "    Args:\n",
        "        obs_space_dims: Dimension of the observation space\n",
        "        action_space_dims: Dimension of the action space\n",
        "    \"\"\"\n",
        "    self.lr = lr\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    self.policy = Policy(observation_space, action_space)\n",
        "    self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.lr)\n",
        "\n",
        "  def sample_action(self, obs_np):\n",
        "    \"\"\"Returns an action, conditioned on the policy and observation.\n",
        "\n",
        "    Args:\n",
        "        obs: Observation from the environment\n",
        "\n",
        "    Returns:\n",
        "        action: Action to be performed\n",
        "    \"\"\"\n",
        "\n",
        "    obs = torch.tensor(obs_np, dtype=torch.float32, device=DEVICE)\n",
        "    mean, std = self.policy(obs)\n",
        "\n",
        "    dist = Normal(mean, std + self.epsilon)\n",
        "    action = dist.sample()\n",
        "\n",
        "    log_prob = dist.log_prob(action).sum()\n",
        "\n",
        "    action = np.clip(action.cpu().numpy(), -1.0, 1.0)\n",
        "\n",
        "    return action, log_prob\n",
        "\n",
        "  def update(self, log_prob, rewards):\n",
        "    \"\"\"Updates the policy network's weights.\n",
        "      Loss = E[Gt * log(p)]\n",
        "    \"\"\"\n",
        "    returns = []\n",
        "    g = 0\n",
        "\n",
        "    for reward in reversed(rewards):\n",
        "      g = reward + self.gamma * g\n",
        "      returns.insert(0, g)\n",
        "\n",
        "    returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "    log_prob = torch.stack(log_prob)\n",
        "    loss = -torch.sum(log_prob * returns)\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "SLE5A8tQzZxI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "KAg_tcNFAKBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import glob\n",
        "\n",
        "# --- Configs ---\n",
        "ENV_ID = \"InvertedPendulum-v5\"\n",
        "MODEL_PATH = \"reinforce_policy.pth\"\n",
        "EVAL_EPISODES = 5\n",
        "VIDEO_FOLDER = \"videos\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load environment and model ---\n",
        "os.makedirs(VIDEO_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Setup environment with video recording ---\n",
        "env = gym.make(ENV_ID, render_mode=\"human\", reset_noise_scale=0.6)\n",
        "\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "\n",
        "policy = Policy(obs_dim, act_dim).to(DEVICE)\n",
        "policy.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "policy.eval()\n",
        "\n",
        "# --- Run Evaluation ---\n",
        "returns = []\n",
        "for ep in range(EVAL_EPISODES):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    ep_return = 0\n",
        "\n",
        "    while not done:\n",
        "        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=DEVICE)\n",
        "        with torch.no_grad():\n",
        "            mu, std = policy(obs_tensor)\n",
        "            action = mu.cpu().numpy()  # use mean (deterministic)\n",
        "\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        ep_return += reward\n",
        "        done = terminated or truncated\n",
        "    returns.append(ep_return)\n",
        "    clear_output(wait=True)\n",
        "\n",
        "env.close()\n",
        "print(f\"\\n✅ Average Return over {EVAL_EPISODES} episodes: {np.mean(returns):.2f}\")"
      ],
      "metadata": {
        "id": "XXNBECbu9tjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make env\n",
        "base_env = gym.make(\"InvertedPendulum-v5\")\n",
        "env = gym.wrappers.RecordEpisodeStatistics(base_env)  # Records episode-reward\n",
        "\n",
        "# init -> episodes, obs, actions\n",
        "episodes = int(1e4)\n",
        "observation_space = env.observation_space.shape[0]\n",
        "action_space = env.action_space.shape[0]\n",
        "\n",
        "agent = REINFORCE(observation_space, action_space)\n",
        "\n",
        "for episode in tqdm(range(1, episodes+1), desc=\"Training\"):\n",
        "  obs, info = env.reset()\n",
        "  done = False\n",
        "  ep_log_probs, ep_rewards = [], []\n",
        "\n",
        "  while not done:\n",
        "    action, log_prob = agent.sample_action(obs)\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    ep_rewards.append(reward)\n",
        "    ep_log_probs.append(log_prob)\n",
        "    done = terminated or truncated\n",
        "\n",
        "  agent.update(ep_log_probs, ep_rewards)\n",
        "\n",
        "  if episode % 1000 == 0:\n",
        "    avg_reward = int(np.mean(env.return_queue))\n",
        "    ep_return = sum(ep_rewards)\n",
        "    print()\n",
        "    print(\"Episode:\", episode, \"Average Reward:\", avg_reward, \"Return: \", ep_return)\n",
        "    torch.save(agent.policy.state_dict(), f\"reinforce_policy_ep{episode}.pth\")\n",
        "    print(\"-\"*50)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "QBPQ9a_x9AWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}