{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ztDLuCpNwFhC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk8FvpuWtpeD",
        "outputId": "83604e3f-7f95-4b22-a78b-4b2cbaccd7db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: mujoco>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.3.2)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.5)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.2.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (2.9.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.22.0)\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "%pip install \"gymnasium[mujoco]\" \"tensorboardX\"\n",
        "import gymnasium as gym\n",
        "from gymnasium.vector import SyncVectorEnv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import MultivariateNormal\n",
        "from tqdm import tqdm\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Memory:   # collected from old policy\n",
        "  def __init__(self):\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "    self.is_terminals = []\n",
        "    self.logprobs = []\n",
        "\n",
        "  def clear_memory(self):\n",
        "    del self.states[:]\n",
        "    del self.actions[:]\n",
        "    del self.rewards[:]\n",
        "    del self.is_terminals[:]\n",
        "    del self.logprobs[:]"
      ],
      "metadata": {
        "id": "B3qKzQNzt0mE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "  def __init__(self, obs_dim, action_dim, action_std):\n",
        "    super(ActorCritic, self).__init__()\n",
        "\n",
        "    self.actor = nn.Sequential(\n",
        "      nn.Linear(obs_dim, 128),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(128, 128),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(128, action_dim),\n",
        "      nn.Tanh()\n",
        "  )\n",
        "\n",
        "    self.critic = nn.Sequential(\n",
        "      nn.Linear(obs_dim, 128),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(128, 128),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(128, 1)\n",
        "    )\n",
        "\n",
        "    self.action_var = torch.full((action_dim, ), action_std * action_std).to(device)    #(4, ) variance of a gaussian dist\n",
        "\n",
        "  def act(self, state, memory):\n",
        "    action_mean = self.actor(state)\n",
        "    cov_mat = torch.diag(self.action_var).to(device) # covariance matrix for multivariate distribution\n",
        "    dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "    action = dist.sample()\n",
        "    logprob = dist.log_prob(action)\n",
        "\n",
        "    memory.states.append(state)\n",
        "    memory.actions.append(action)\n",
        "    memory.logprobs.append(logprob)\n",
        "\n",
        "    return action.detach()\n",
        "\n",
        "  def evaluate(self, state, action):\n",
        "    state_value = self.critic(state)    # (4000, 1)\n",
        "\n",
        "    # to calculate action score(logprobs) and distribution entropy\n",
        "    action_mean = self.actor(state)                     # (4000,4)\n",
        "    action_var = self.action_var.expand_as(action_mean) # (4000,4)\n",
        "    cov_mat = torch.diag_embed(action_var).to(device)   # (4000,4,4)\n",
        "    dist = MultivariateNormal(action_mean, cov_mat)\n",
        "    action_logprobs = dist.log_prob(action.unsqueeze(1)) # Unsqueeze action to match expected shape\n",
        "    dist_entropy = dist.entropy()\n",
        "\n",
        "    return action_logprobs, torch.squeeze(state_value), dist_entropy"
      ],
      "metadata": {
        "id": "ePmiGxc-t325"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO:\n",
        "  def __init__(self, obs_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip, restore=False, ckpt=None):\n",
        "    self.lr = lr\n",
        "    self.betas = betas\n",
        "    self.gamma = gamma\n",
        "    self.eps_clip = eps_clip\n",
        "    self.K_epochs = K_epochs\n",
        "\n",
        "    self.policy = ActorCritic(obs_dim, action_dim, action_std).to(device)\n",
        "    if restore and ckpt:\n",
        "      pretained_model = torch.load(ckpt, map_location=lambda storage, loc: storage)\n",
        "      self.policy.load_state_dict(pretained_model)\n",
        "    self.old_policy = ActorCritic(obs_dim, action_dim, action_std).to(device)\n",
        "    self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "    self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "    self.MSE = nn.MSELoss()\n",
        "\n",
        "  def get_action(self, state, memory):\n",
        "    # reshape(1, -1) turns it into shape (1, 4) from (4,)\n",
        "    state = torch.FloatTensor(state.reshape(1, -1)).to(device).float()\n",
        "    return self.old_policy.act(state, memory).cpu().numpy().flatten()\n",
        "\n",
        "  def update(self, memory):\n",
        "    returns = []\n",
        "    g = 0\n",
        "\n",
        "    for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "      if is_terminal:\n",
        "        g = 0\n",
        "      g = reward + (self.gamma * g)\n",
        "      returns.insert(0, g)\n",
        "\n",
        "    returns = torch.tensor(returns).to(device).float() # Ensure returns is float32\n",
        "\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
        "\n",
        "    old_states = torch.squeeze(torch.stack(memory.states).to(device)).detach().float() # Ensure old_states is float32\n",
        "    old_actions = torch.squeeze(torch.stack(memory.actions).to(device)).detach()\n",
        "    old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).to(device).detach()\n",
        "\n",
        "    for _ in range(self.K_epochs):\n",
        "      logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "      # importance ratio\n",
        "      ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "      # advantages\n",
        "      advantages = returns - state_values.detach()\n",
        "\n",
        "      # actor_loss\n",
        "      surr1 = ratios * advantages\n",
        "      surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "      actor_loss = -torch.min(surr1, surr2)\n",
        "\n",
        "      # critic_loss\n",
        "      critic_loss = 0.5 * self.MSE(state_values, returns)\n",
        "\n",
        "      # loss\n",
        "      loss = actor_loss + critic_loss - 0.001 * dist_entropy\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "    # copy new weights into old_policy\n",
        "    self.old_policy.load_state_dict(self.policy.state_dict())"
      ],
      "metadata": {
        "id": "3h6YKz82wFxF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name=\"InvertedDoublePendulum-v5\"\n",
        "env = gym.make(env_name, reset_noise_scale=0.1)\n",
        "\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_std = 0.5\n",
        "lr = 1e-4\n",
        "K_epochs = 6\n",
        "eps_clip = 0.3\n",
        "gamma = 0.99\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "max_episodes = int(1e5)\n",
        "max_timesteps = 2000\n",
        "update_timestep = 8000\n",
        "solved_reward = 9200\n",
        "\n",
        "render = False\n",
        "print_interval = 500\n",
        "save_interval = 500\n",
        "writer = SummaryWriter()\n",
        "tb = True\n",
        "\n",
        "\n",
        "# Train Loop\n",
        "memory = Memory()\n",
        "agent = PPO(obs_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
        "\n",
        "r_reward = time_step = avg_length = 0\n",
        "\n",
        "for ep in tqdm(range(1, max_episodes + 1), desc=\"Training\"):\n",
        "  state, _ = env.reset()\n",
        "  x = 1\n",
        "  for t in range(1, max_timesteps + 1):\n",
        "    time_step += 1\n",
        "\n",
        "    # run policy\n",
        "    action = agent.get_action(state, memory)\n",
        "\n",
        "    state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated\n",
        "\n",
        "    memory.rewards.append(reward)\n",
        "    memory.is_terminals.append(done)\n",
        "\n",
        "    if time_step % update_timestep == 0:\n",
        "      agent.update(memory)\n",
        "      memory.clear_memory()\n",
        "      time_step = 0\n",
        "\n",
        "    r_reward += reward\n",
        "\n",
        "    x = t\n",
        "    if render: env.render()\n",
        "    if done: break\n",
        "  avg_length += x\n",
        "\n",
        "  if r_reward > (print_interval * solved_reward):\n",
        "    print(\"########## Solved! ##########\")\n",
        "    torch.save(agent.policy.state_dict(), 'PPO_continuous_{}.pth'.format(env_name))\n",
        "    print('Save a checkpoint!')\n",
        "    break\n",
        "\n",
        "  if ep % save_interval == 0:\n",
        "      torch.save(agent.policy.state_dict(), 'PPO_continuous_{}.pth'.format(env_name))\n",
        "      print('Save a checkpoint!')\n",
        "\n",
        "  if ep % print_interval == 0:\n",
        "      avg_length = int(avg_length / print_interval)\n",
        "      r_reward = int((r_reward / print_interval))\n",
        "\n",
        "      print('Episode {} \\t Avg length: {} \\t Avg reward: {}'.format(ep, avg_length, r_reward))\n",
        "\n",
        "      if tb:\n",
        "        writer.add_scalar('scalar/reward', r_reward, ep)\n",
        "        writer.add_scalar('scalar/length', avg_length, ep)\n",
        "\n",
        "      r_reward, avg_length = 0, 0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1IAKMvQyb2h",
        "outputId": "df3cb60a-869d-4df5-ddd9-933d4495a7c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/100000 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "ztDLuCpNwFhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name=\"InvertedDoublePendulum-v5\"\n",
        "env = gym.make(env_name, reset_noise_scale=0.1)\n",
        "\n",
        "MODEL_PATH = \"PPO_continuous_InvertedDoublePendulum-v5.pth\"\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_std = 0.5\n",
        "lr = 3e-3\n",
        "K_epochs = 80\n",
        "eps_clip = 0.2\n",
        "gamma = 0.99\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "memory = Memory()\n",
        "\n",
        "agent = PPO(obs_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip, True, MODEL_PATH)\n",
        "\n",
        "episode_reward, time_step = 0, 0\n",
        "avg_episode_reward, avg_length = 0, 0\n",
        "test_episodes = 100\n",
        "# test\n",
        "for i_episode in range(1, test_episodes+1):\n",
        "    state, _ = env.reset()\n",
        "    while True:\n",
        "        time_step += 1\n",
        "\n",
        "        # Run old policy\n",
        "        action = agent.get_action(state, memory)\n",
        "\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "        if done:\n",
        "            print('Episode {} \\t Length: {} \\t Reward: {}'.format(i_episode, time_step, episode_reward))\n",
        "            avg_episode_reward += episode_reward\n",
        "            avg_length += time_step\n",
        "            memory.clear_memory()\n",
        "            time_step, episode_reward = 0, 0\n",
        "            break\n",
        "\n",
        "print('Test {} episodes DONE!'.format(test_episodes))\n",
        "print('Avg episode reward: {} | Avg length: {}'.format(avg_episode_reward/test_episodes, avg_length/test_episodes))"
      ],
      "metadata": {
        "id": "0kvfX5UCOdgH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}