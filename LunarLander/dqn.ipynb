{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-ZS1JbLd_yk",
        "outputId": "6a824316-6a1b-428e-d487-5c6d5b8619e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.1\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.5)\n",
            "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379415 sha256=f93ddd2a3d448d80d01606b92d99ede791e35f4ccaf240e9327c92ff828c91a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, tensorboardX\n",
            "Successfully installed box2d-py-2.3.5 tensorboardX-2.6.4\n"
          ]
        }
      ],
      "source": [
        "%pip install \"swig\"\n",
        "%pip install \"gymnasium[box2d]\" \"tensorboardX\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from itertools import count\n",
        "from tensorboardX import SummaryWriter\n",
        "import random\n",
        "from collections import namedtuple, deque"
      ],
      "metadata": {
        "id": "QF_AroeteC1b"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self,\n",
        "               state_dims,\n",
        "               action_dims,\n",
        "               activation_fn=F.relu\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer1 = nn.Linear(state_dims, 256)\n",
        "    self.layer2 = nn.Linear(256, 256)\n",
        "    self.layer3 = nn.Linear(256, action_dims)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = F.relu(self.layer1(x))\n",
        "      x = F.relu(self.layer2(x))\n",
        "      return self.layer3(x)"
      ],
      "metadata": {
        "id": "cy9ISCHdeGkf"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self, n_actions, memory_size, batch_size):\n",
        "        self.n_actions = n_actions\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen = memory_size)\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "\n",
        "        states = torch.cat(states)\n",
        "        next_states = torch.cat(next_states)\n",
        "        actions = torch.cat(actions)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "        dones = torch.tensor(dones, dtype=torch.bool, device=device).unsqueeze(1)\n",
        "\n",
        "\n",
        "        # print(states.shape, actions.shape, rewards.shape, next_states.shape, dones.shape)\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "class DQN:\n",
        "  def __init__(self, state_dims, action_dims, memory_capacity, batch_size, epsilon, gamma, lr, tau):\n",
        "    super().__init__()\n",
        "    self.q_net = Net(state_dims, action_dims)\n",
        "    self.target_net = Net(state_dims, action_dims)\n",
        "    self.optimizer = optim.AdamW(self.q_net.parameters(), lr=lr)\n",
        "\n",
        "    self.memory = ReplayBuffer(action_dims, memory_capacity, batch_size)\n",
        "\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.tau = tau\n",
        "\n",
        "    self.learn_counter = 0\n",
        "\n",
        "    self.MSE = nn.MSELoss()\n",
        "    self.Q_NETWORK_ITERATION = 5\n",
        "\n",
        "  def save(self, experiences):\n",
        "    self.memory.add(*experiences)\n",
        "    self.learn_counter += 1\n",
        "    if self.learn_counter % self.Q_NETWORK_ITERATION == 0:\n",
        "      if len(self.memory) > self.memory.batch_size:\n",
        "        experiences = self.memory.sample()\n",
        "        self.learn(experiences)\n",
        "\n",
        "  def learn(self, experiences):\n",
        "    states, actions, rewards, next_states, is_terminals = experiences\n",
        "\n",
        "    q_target = (self.gamma * self.target_net(next_states).detach().max(1)[0].unsqueeze(1) * ~is_terminals) + rewards\n",
        "\n",
        "    q_eval = self.q_net(states).gather(1, actions)\n",
        "\n",
        "    loss = self.MSE(q_eval, q_target)\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), max_norm=1.0)\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # soft update\n",
        "    self.softUpdate()\n",
        "  def softUpdate(self):\n",
        "    for eval_param, target_param in zip(self.q_net.parameters(), self.target_net.parameters()):\n",
        "      target_param.data.copy_(self.tau*eval_param.data + (1.0-self.tau)*target_param.data)\n",
        "\n",
        "  def select_action(self, state, env):\n",
        "    if random.random() > self.epsilon: # eps greedy\n",
        "      with torch.no_grad():\n",
        "        action = self.q_net(state).max(1).indices.view(1, 1)\n",
        "    else: # random\n",
        "      action = torch.tensor([[env.action_space.sample()]], dtype=torch.long, device=device)\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "id": "3q96QpFreVLL"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "LR = 1e-4\n",
        "GAMMA = 0.99\n",
        "EPSILON = 1.0\n",
        "EPSILON_END = 0.01\n",
        "MEMORY_CAPACITY = 50000\n",
        "Q_NETWORK_ITERATION = 100\n",
        "LEARN_STEP = 5\n",
        "TAU = 0.01\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\")\n",
        "NUM_ACTIONS = env.action_space.n\n",
        "NUM_STATES = env.observation_space.shape[0]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "agent = DQN(NUM_STATES, NUM_ACTIONS, MEMORY_CAPACITY, BATCH_SIZE, EPSILON, GAMMA, LR, TAU)\n",
        "tb_writer = SummaryWriter()\n",
        "max_eps = 10000\n",
        "ep_rewards = []\n",
        "\n",
        "solved_reward   = 200\n",
        "print_interval  = 100\n",
        "\n",
        "running_reward = 0.0\n",
        "\n",
        "for ep in tqdm(range(1, max_eps + 1), desc=\"TRAINING\"):\n",
        "  state, _ = env.reset()\n",
        "  state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "  done = False\n",
        "  ep_reward = 0.0\n",
        "\n",
        "  for step in count():\n",
        "    action = agent.select_action(state, env)\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
        "\n",
        "    ep_reward += reward\n",
        "    done = terminated or truncated\n",
        "    reward = torch.tensor([reward], device=device)\n",
        "    next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    agent.save((state, action, reward, next_state, done))\n",
        "    state = next_state\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "\n",
        "  ep_rewards.append(ep_reward)\n",
        "  running_reward += ep_reward\n",
        "  agent.epsilon = max(agent.epsilon * 0.995, EPSILON_END)\n",
        "  if ep % print_interval == 0:\n",
        "    avg_ep_reward = running_reward / print_interval\n",
        "\n",
        "    print(f\"Episode {ep:5d}  \"\n",
        "          f\"Avg reward: {avg_ep_reward:.2f}\")\n",
        "\n",
        "    if tb_writer is not None:\n",
        "        tb_writer.add_scalar('Reward', avg_ep_reward, ep)\n",
        "    if avg_ep_reward > solved_reward:\n",
        "        print(\"\\n########## Solved! ##########\")\n",
        "        break\n",
        "\n",
        "    running_reward = 0.0\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAUjB0hClHOK",
        "outputId": "bb95d2eb-a87e-4be3-f411-a9b6c6c6e770"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAINING:   1%|          | 100/10000 [00:13<25:02,  6.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode   100  Avg reward: -162.81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAINING:   2%|▏         | 200/10000 [00:58<2:39:26,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode   200  Avg reward: -67.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAINING:   3%|▎         | 300/10000 [03:02<3:35:48,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode   300  Avg reward: -18.54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAINING:   4%|▍         | 400/10000 [04:43<1:47:25,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode   400  Avg reward: 77.47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAINING:   5%|▌         | 500/10000 [05:58<2:00:50,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode   500  Avg reward: 199.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAINING:   6%|▌         | 599/10000 [07:07<1:51:44,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode   600  Avg reward: 202.06\n",
            "\n",
            "########## Solved! ##########\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}