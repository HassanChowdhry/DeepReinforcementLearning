{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrrGHGhbuvoFVPuRF8bQ5S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HassanChowdhry/DeepReinforcementLearning/blob/main/PPO_InvertedPendulum_Mujoco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk8FvpuWtpeD",
        "outputId": "82b05c55-35a4-49bc-f387-6b7947100461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: mujoco>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.3.2)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.5)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.2.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (2.9.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.22.0)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "%pip install \"gymnasium[mujoco]\" \"tensorboardX\"\n",
        "import gymnasium as gym\n",
        "from gymnasium.vector import SyncVectorEnv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import MultivariateNormal\n",
        "from tqdm import tqdm\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Memory:   # collected from old policy\n",
        "  def __init__(self):\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "    self.is_terminals = []\n",
        "    self.logprobs = []\n",
        "\n",
        "  def clear_memory(self):\n",
        "    del self.states[:]\n",
        "    del self.actions[:]\n",
        "    del self.rewards[:]\n",
        "    del self.is_terminals[:]\n",
        "    del self.logprobs[:]"
      ],
      "metadata": {
        "id": "B3qKzQNzt0mE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "  def __init__(self, obs_dim, action_dim, action_std):\n",
        "    super(ActorCritic, self).__init__()\n",
        "\n",
        "    self.actor = nn.Sequential(\n",
        "      nn.Linear(obs_dim, 64),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(32, action_dim),\n",
        "      nn.Tanh()\n",
        "  )\n",
        "\n",
        "    self.critic = nn.Sequential(\n",
        "      nn.Linear(obs_dim, 64),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "    self.action_var = torch.full((action_dim, ), action_std * action_std).to(device)    #(4, ) variance of a gaussian dist\n",
        "\n",
        "  def act(self, state, memory):\n",
        "    action_mean = self.actor(state)\n",
        "    cov_mat = torch.diag(self.action_var).to(device) # covariance matrix for multivariate distribution\n",
        "    dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "    action = dist.sample()\n",
        "    logprob = dist.log_prob(action)\n",
        "\n",
        "    memory.states.append(state)\n",
        "    memory.actions.append(action)\n",
        "    memory.logprobs.append(logprob)\n",
        "\n",
        "    return action.detach()\n",
        "\n",
        "  def evaluate(self, state, action):\n",
        "    state_value = self.critic(state)    # (4000, 1)\n",
        "\n",
        "    # to calculate action score(logprobs) and distribution entropy\n",
        "    action_mean = self.actor(state)                     # (4000,4)\n",
        "    action_var = self.action_var.expand_as(action_mean) # (4000,4)\n",
        "    cov_mat = torch.diag_embed(action_var).to(device)   # (4000,4,4)\n",
        "    dist = MultivariateNormal(action_mean, cov_mat)\n",
        "    action_logprobs = dist.log_prob(action.unsqueeze(1)) # Unsqueeze action to match expected shape\n",
        "    dist_entropy = dist.entropy()\n",
        "\n",
        "    return action_logprobs, torch.squeeze(state_value), dist_entropy"
      ],
      "metadata": {
        "id": "ePmiGxc-t325"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO:\n",
        "  def __init__(self, obs_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip):\n",
        "    self.lr = lr\n",
        "    self.betas = betas\n",
        "    self.gamma = gamma\n",
        "    self.eps_clip = eps_clip\n",
        "    self.K_epochs = K_epochs\n",
        "\n",
        "    self.policy = ActorCritic(obs_dim, action_dim, action_std).to(device)\n",
        "    self.old_policy = ActorCritic(obs_dim, action_dim, action_std).to(device)\n",
        "    self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "    self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "    self.MSE = nn.MSELoss()\n",
        "\n",
        "  def get_action(self, state, memory):\n",
        "    # reshape(1, -1) turns it into shape (1, 4) from (4,)\n",
        "    state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "    return self.old_policy.act(state, memory).cpu().numpy().flatten()\n",
        "\n",
        "  def update(self, memory):\n",
        "    returns = []\n",
        "    g = 0\n",
        "\n",
        "    for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "      if is_terminal:\n",
        "        g = 0\n",
        "      g = reward + (self.gamma * g)\n",
        "      returns.insert(0, g)\n",
        "\n",
        "    returns = torch.tensor(returns).to(device)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
        "\n",
        "    old_states = torch.squeeze(torch.stack(memory.states).to(device)).detach()\n",
        "    old_actions = torch.squeeze(torch.stack(memory.actions).to(device)).detach()\n",
        "    old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).to(device).detach()\n",
        "\n",
        "    for _ in range(self.K_epochs):\n",
        "      logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "      # importance ratio\n",
        "      ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "      # advantages\n",
        "      advantages = returns - state_values.detach()\n",
        "\n",
        "      # actor_loss\n",
        "      surr1 = ratios * advantages\n",
        "      surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "      actor_loss = -torch.min(surr1, surr2)\n",
        "\n",
        "      # critic_loss\n",
        "      critic_loss = 0.5 * self.MSE(state_values, returns)\n",
        "\n",
        "      # loss\n",
        "      loss = actor_loss + critic_loss - 0.001 * dist_entropy\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "    # copy new weights into old_policy\n",
        "    self.old_policy.load_state_dict(self.policy.state_dict())\n"
      ],
      "metadata": {
        "id": "3h6YKz82wFxF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name=\"InvertedPendulum-v5\"\n",
        "env = gym.make(env_name, reset_noise_scale=0.1)\n",
        "\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_std = 0.5\n",
        "lr = 3e-3\n",
        "K_epochs = 80\n",
        "eps_clip = 0.2\n",
        "gamma = 0.99\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "max_episodes = int(1e4)\n",
        "max_timesteps = 1500\n",
        "update_timestep = 4000\n",
        "solved_reward = 1000\n",
        "\n",
        "render = False\n",
        "print_interval = 500\n",
        "save_interval = 500\n",
        "writer = SummaryWriter()\n",
        "tb = True\n",
        "\n",
        "\n",
        "# Train Loop\n",
        "memory = Memory()\n",
        "agent = PPO(obs_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
        "\n",
        "r_reward = time_step = avg_length = 0\n",
        "\n",
        "for ep in tqdm(range(1, max_episodes + 1), desc=\"Training\"):\n",
        "  state, _ = env.reset()\n",
        "  x = 1\n",
        "  for t in range(1, max_timesteps + 1):\n",
        "    time_step += 1\n",
        "\n",
        "    # run policy\n",
        "    action = agent.get_action(state, memory)\n",
        "\n",
        "    state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    memory.rewards.append(reward)\n",
        "    memory.is_terminals.append(done)\n",
        "\n",
        "    if time_step % update_timestep == 0:\n",
        "      agent.update(memory)\n",
        "      memory.clear_memory()\n",
        "      time_step = 0\n",
        "\n",
        "    r_reward += reward\n",
        "\n",
        "    x = t\n",
        "    if render: env.render()\n",
        "    if done: break\n",
        "  avg_length += x\n",
        "\n",
        "  if r_reward > (print_interval * solved_reward):\n",
        "    print(\"########## Solved! ##########\")\n",
        "    torch.save(agent.policy.state_dict(), 'PPO_continuous_{}.pth'.format(env_name))\n",
        "    print('Save a checkpoint!')\n",
        "    break\n",
        "\n",
        "  if ep % save_interval == 0:\n",
        "      torch.save(agent.policy.state_dict(), '/PPO_continuous_{}.pth'.format(env_name))\n",
        "      print('Save a checkpoint!')\n",
        "\n",
        "  if ep % print_interval == 0:\n",
        "      avg_length = int(avg_length / print_interval)\n",
        "      running_reward = int((r_reward / print_interval))\n",
        "\n",
        "      print('Episode {} \\t Avg length: {} \\t Avg reward: {}'.format(ep, avg_length, running_reward))\n",
        "\n",
        "      if tb:\n",
        "        writer.add_scalar('scalar/reward', running_reward, ep)\n",
        "        writer.add_scalar('scalar/length', avg_length, ep)\n",
        "\n",
        "      running_reward, avg_length = 0, 0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1IAKMvQyb2h",
        "outputId": "e489e0a3-3f50-4f5e-dde6-0e969fd88ee6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   5%|▌         | 509/10000 [00:07<02:49, 56.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save a checkpoint!\n",
            "Episode 500 \t Avg length: 9 \t Avg reward: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  10%|█         | 1003/10000 [00:24<07:43, 19.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save a checkpoint!\n",
            "Episode 1000 \t Avg length: 17 \t Avg reward: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  15%|█▌        | 1501/10000 [06:59<2:20:16,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save a checkpoint!\n",
            "Episode 1500 \t Avg length: 504 \t Avg reward: 528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  18%|█▊        | 1815/10000 [13:44<1:01:57,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########## Solved! ##########\n",
            "Save a checkpoint!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ztDLuCpNwFhC"
      }
    }
  ]
}